{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from nilearn import plotting\n",
    "from fragmenter import Fragment\n",
    "from fragmenter import adjacency\n",
    "from plotnine import *\n",
    "from os.path import join\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple analysis of the performance of the algorithms used for parcellation. This includes computing the speed of each, as well as checking the variation of the parcellations n number of times. To see how each algorithm partitions the brain, refer to the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data\n",
    "# Prep the surface and fragmentation object\n",
    "surfPath = join('..','data','freesurfer','fsaverage','surf','rh.inflated')\n",
    "testSurface = nb.freesurfer.read_geometry(surfPath)\n",
    "\n",
    "# load timing and similarity data\n",
    "timing = pd.read_csv('TimingData.csv')\n",
    "simLbls10 = np.load('similarityLabels10.npy')\n",
    "simLbls100 = np.load('similarityLabels100.npy')\n",
    "methods = np.repeat(['gmm', 'k_means', 'ward'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to run each clustering\n",
    "First we have the effect of cluster size on time. These are computed on the fsaverage surface with ~160k vertices, as it denotes a 'worst case scenario' of sorts. The plot suggests that the Ward algorithm is very stable in its computation time, but it is the slowest for small parcellations. Conversely, GMM and K-means have increasing time penalties, with the former having a sharper decrease in performance. Overall, in terms on computational demands, it seems like K-means would be the best option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cluster size by time data\n",
    "ggplot(aes(x='size', y='time', color = 'method'), data=timing) +\\\n",
    "scale_y_log10() +\\\n",
    "geom_point(size = 2) +\\\n",
    "xlab('Cluster Size') +\\\n",
    "ylab('Time (log-seconds)') +\\\n",
    "theme_classic()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of the partitions\n",
    "However, while K-means is the fastest, it is important to see how stable are the partitions produced by each algorithm. To determine this, we estimated parcels with each algorithm 10 times, and used the adjusted rand index (RI) to compute the level of agreement between any 2 parcellations across methods. The adjusted rand index estimates the rate at which every pair of nodes in partition A were equally assigned to the same/different parcels on partition B, over all possible changes (e.g. clustered together on A, but separately on B). Because of this, the adjusted rand index is a simple metric that determines the percentage agreement of any two partitions that is agnostic to the labeling scheme.\n",
    "\n",
    "The similarity matrix below shows the rand index (with 1 being perfect agreement) for every pairwise comparison across methods. All partitions include 10 parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity matrix\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plotting.plot_matrix(simLbls10, vmin=0, reorder=True, labels=methods, figure=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that partitions tend to be stable within, but not across methods (most within-method RI > 0.5). Specifically, Ward is clearly perfectly stable, GMM is mostly stable (with one exception due to some small random initialization element in the algorithm), and K-means is relatively variable. If stability is a desired feature of the parcellations, then the speed of K-means might not be worth it. \n",
    "\n",
    "However, this was a small partition (10 parcels). The similarity matrix below shows the same approach, but estimating 100 clusters per partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity matrix with 100 clusters\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plotting.plot_matrix(simLbls100, vmin=0, vmax=1, reorder=True, labels=methods, figure=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that Ward is extremely consistent (while retaining the computing time from smaller clusterings, see fig 1). GMM, on the other hand, becomes slightly more variable. This is just to demonstrate the tradeoffs of each method, and the user will have to identify the parameter combination that best fits their needs.\n",
    "\n",
    "One last thing to mention is the lack of spectral clustering in this data. This method relies on eigenvalue decomposition of a distance-based adjacency matrix, which for ~160k vertices is very computationally demanding. This method, however, is also very stable, and so we provide it just in case. Downsampling of the original cortical mesh to just a few vertices would be ideal to take advantage of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code used to generate the data (timing is slow)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Timing\n",
    "# # Set up cluster sizes (log better?)\n",
    "# # clustSize = [5,50,150,300]\n",
    "# clustSize = np.round(np.logspace(1,3, num=20))\n",
    "# clustSize = clustSize.astype(int)\n",
    "# methods = ['gmm', 'k_means', 'ward']\n",
    "\n",
    "# tottime = []\n",
    "# allLabels = []\n",
    "\n",
    "# for cluster in clustSize:\n",
    "#     print('Current cluster: ' + str(cluster))\n",
    "#     testFragment = Fragment.Fragment(n_clusters=cluster)\n",
    "#     for method in methods:\n",
    "#         print('Current method: ' + method)\n",
    "#         ST = time.time()\n",
    "#         testFragment.fit(testSurface[0], testSurface[1], method = method)\n",
    "#         allLabels.append(testFragment.label_)\n",
    "#         tottime.append(time.time() - ST)\n",
    "#\n",
    "# timing = pd.DataFrame(dict(size=np.repeat(clustSize, len(methods)), \n",
    "#                            method= methods * len(clustSize),\n",
    "#                           time = tottime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Similarity\n",
    "# methods = np.repeat(['gmm', 'k_means', 'ward'], 10)\n",
    "# allLabels100 = []\n",
    "  \n",
    "# for method in methods:\n",
    "#     testFragment = Fragment.Fragment(n_clusters=100)\n",
    "#     print('Current method: ' + method)\n",
    "#     testFragment.fit(testSurface[0], testSurface[1], method = method)\n",
    "#     allLabels100.append(testFragment.label_)\n",
    "#\n",
    "# simLbls100 = np.zeros((30, 30))\n",
    "#\n",
    "# for count, i in enumerate(allLabels100):\n",
    "#     for count2, j in enumerate(allLabels100):\n",
    "#         simLbls100[count,count2] = adjusted_rand_score(i,j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
